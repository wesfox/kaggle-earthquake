{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Heartquake\n",
    "\n",
    "Ce notebook se base sur le projet Kaggle Hearthquake : https://www.kaggle.com/c/LANL-Earthquake-Prediction/overview\n",
    "\n",
    "Le travail a été produit par Nicolas Fley, Romain Pelloie et Julien Raspaud dans le cadre du projet DaC (Données à la Connaissances).\n",
    "\n",
    "Ce projet a été choisis pour deux principales raisons :\n",
    "\n",
    "- Le fait qu'il se base sur des \"times series\", type de données auquel nous n'avions pas encore été confrontés.\n",
    "- Le fait que ce soit un projet Kaggle avec prix à la clé, en effet cela nous assure la qualité des données d'entrainement.\n",
    "\n",
    "Il est composé de quatres parties :\n",
    "\n",
    "- Préambules (librairies et informations générales)\n",
    "- Découverte du dataset d'entrainement et de test\n",
    "- Extraction des features et entrainement\n",
    "- Submission\n",
    "- Amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préambule \n",
    "#### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilitaire de listage de fichier dans des répertoires\n",
    "import glob\n",
    "\n",
    "# librairies de manipulation de données\n",
    "import pandas as pd # utilisé pour lire les csv et créer des dataframe.\n",
    "import numpy as np\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# features\n",
    "from tqdm import tqdm # permet d'afficher l'avancement de l'extraction des features\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# model d'entrainement\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# affichage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seismologie\n",
    "\n",
    "Nous avons compris le fonctionnement des séismes et le lien avec les `ondes acoustiques` par le biais de cette vidéos : https://www.youtube.com/watch?v=uA_OLKfQpYA\n",
    "\n",
    "Nous apprenons qu'un seisme est détecté par 3 grosses variations acoustiques. Il y a d'abord des P-Wave (Primary/Pressure Wave) puis des S-Wave (Secondary Wave) avant d'arriver aux Surface Wave qui sont les plus fortes et celles ressenti à la surfaces par les habitants. Ces 3 ondes sont surtout utilisés pour localiser l'epicentre des seismes et les P-Wave peuvent dans une certaines mesure être detectés pour prévenir la population quelques minutes avant que les ondes surfacique soient ressentis.\n",
    "\n",
    "Néanmoins, ce projet n'est pas focalisés sur ces 3 types d'ondes, en effet, le dataset est issue de mesures effectuées expérimentalement en laboratoire. Nous devons déterminer, en se basant sur l'ondes acoustique relevé, le temps qu'il reste avant le prochain séisme.\n",
    "\n",
    "Contrairement à ce que nous pension au départ, le but n'est donc pas de détecter ces 3 ondes mais d'utiliser les ondes acoustiques relevés par le seismographe du laboratoire afin d'en extraire des features ou des motifs permettant de détecter avec précision le moment de la prochaine rupture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Découverte du dataset d'entrainement et de test\n",
    "\n",
    "Le dataset fourni est très gros, il fait 10Go. Nous avons donc commencé par extraire les 10_000_000 premières valeurs du dataset (153 Mo) afin de pouvoir les traiter plus facilement.\n",
    "\n",
    "#### Généralités sur les valeurs\n",
    "\n",
    "Ce dataset contient deux colonnes.\n",
    "La première colonne est nommée `acoustic_data` qui contient les valeurs entières dont la répartition reste à définir. On nottera néanmoins grâce à la seconde cellule représentant 50 valeurs qu'elles sont réparti et ne suivent pas de motif à première vue.\n",
    "La seconde colonne est nommée `time_to_failure` et contient une valeur flottante faisant référence au temps restant avant la prochaine rupture.\n",
    "\n",
    "On notera qu'il se passe environ une nanoseconde entre deux mesure, l'échantillonnage est d'environ 1Ghz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data = pd.read_csv(file_name,dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_data(\"data/train_small.csv\")\n",
    "print(f'Taille du small dataset : {len(small_data)}')\n",
    "pd.set_option('float_format', '{:.11}'.format)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(small_data['acoustic_data'][:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichage et répartition temporelle\n",
    "\n",
    "Nous avons affiché les valeurs celon différentes échelles, après plusieurs tests nous avons remarqués que le dataset était constitué d'une multitude d'ensembles de 4096 points espacés dans le temps. Nous allons parcourir les différentes échelles et en extraires les informations utiles.\n",
    "\n",
    "- **200 points** (~200ns) : pas de motifs apparant, le signal semble très bruité à cette échelle.\n",
    "- **4096 points** (~4us) : le signal semble plus interessant, des valeurs semblent pouvoir être extraites de ce type de signal.\n",
    "- **next 4096 points** (~4us) : idem, on notera un décalage d'une milliseconde avec le précédent signal, il n'y a pas de lien direct visible avec les 4096 points précédentts\n",
    "- **20_000 points** (~5ms) : on détecte sur ce graphique la mesure, effectuée toutes les millisecondes des 4096 points.\n",
    "- **time to failure** (~2.5s) : ce graphique représente la variation de la grandeur `time to failure`, on notera que l'on commence à environ 2s, que l'on descend à 0 et que l'on reprend à 12 pour descendre vers 10s.\n",
    "- **10_000_000 points** (~2.5s) : on note une grosse amplitude aux alentours des 0ms, on voit aussi les limites de cet affichage. En effet on se rend compte qu'avec ce principe, on lit les points à l'envers (car on utilise le `time to failure` comme coordonnée. On utilisera donc dès à présent une abscisse générale et on mettra le `time to failure` en avant sur l'`acoustic data` comme représenté dans la dernière figure.\n",
    "\n",
    "Les 10_000_000 de points forment 2441 ensembles de 4096 points, sur cet ensemble, une rupture à eu lieu, le `time to failure` reprenant à une valeur beaucoup plus haute.\n",
    "\n",
    "Nous pensons donc intuitivement qu'il nous faudrait utiliser ces ensemble de 4096 points afin de déterminer le temps restant avant la prochaine rupture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.gca().set_title('200 dots')\n",
    "plt.plot(small_data['time_to_failure'][:200],small_data['acoustic_data'][:200])\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.gca().set_title('4096 dots')\n",
    "plt.plot(small_data['time_to_failure'][:4095],small_data['acoustic_data'][:4095])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.gca().set_title('next 4096 dots')\n",
    "plt.plot(small_data['time_to_failure'][4095:4095+4096],small_data['acoustic_data'][4095:4095+4096])\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.gca().set_title('20_000 dots')\n",
    "plt.plot(small_data['time_to_failure'][:20000],small_data['acoustic_data'][:20000])\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.gca().set_title('time to failure')\n",
    "plt.plot(small_data['time_to_failure'][:])\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.gca().set_title('10_000_000 dots')\n",
    "plt.plot(small_data['time_to_failure'][:],small_data['acoustic_data'][:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(small_data['acoustic_data'][:])\n",
    "ax1.set_ylabel('acoustic_data')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(small_data['time_to_failure'][:], color='orange')\n",
    "ax2.set_ylabel('time_to_failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution\n",
    "\n",
    "En enlevant les valeurs extrêmes, on voit que les valeurs suivent une répartition normal centrée en 4,5 et d'écart type 17,8. Faire un scaling sur l'ensemble de ces valeurs est donc une option à envisager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sorted(small_data['acoustic_data'])[100_000:9_900_000],\n",
    "         bins = 100)\n",
    "print(f\"moyenne : {small_data['acoustic_data'].mean()}, variance : {small_data['acoustic_data'].std()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features et entrainement\n",
    "\n",
    "Nous avons défini une fonction nous permettant d'extraire un certain nombre de features composant chacune une agregation des valeurs d'un ensemble donnée.\n",
    "\n",
    "On extrait ainsi pour chaque ensemble :\n",
    "- la moyenne\n",
    "- l'écart type\n",
    "- la mesure de kurtosis (coefficient d’aplatissement, https://fr.wikipedia.org/wiki/Kurtosis)\n",
    "- les valeur max, min, et la valeur max en absolu\n",
    "- 6 quantiles 0.01, 0.05, 0.1, 0.9, 0.95, 0.99\n",
    "\n",
    "On notera une adaptation de cette fonction permettant de générer selon la volonté de l'utilisateur un ensemble de test ou encore retourner seulement les ensembles de données acoustiques processés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(data, rows=4_096, is_not_submission=True):\n",
    "    # create the segments (each row gets a segment value), no need to randomize\n",
    "    segments = int(np.floor(data.shape[0] / rows))\n",
    "    \n",
    "    # first, create dataframes for X and Y\n",
    "    X = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['ave', 'std','kurt', 'max', 'min','abs_max',\n",
    "                                'quantile_095','quantile_099','quantile_09',\n",
    "                                'quantile_01','quantile_005','quantile_001'])\n",
    "    if is_not_submission:\n",
    "        Y = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                               columns=['time_to_failure'])\n",
    "    \n",
    "    # split the datasettt into segments to perform agregation calculation on them \n",
    "    # should be overlaping segments\n",
    "    for segment in tqdm(range(segments)):\n",
    "        seg = data.iloc[segment*rows:segment*rows+rows]\n",
    "        x = seg['acoustic_data'].values\n",
    "        if is_not_submission:\n",
    "            y = seg['time_to_failure'].values[-1]\n",
    "\n",
    "            # time to failure \n",
    "            Y.loc[segment, 'time_to_failure'] = y\n",
    "        \n",
    "        # all our features\n",
    "        X.loc[segment, 'ave'] = x.mean()\n",
    "        X.loc[segment, 'kurt'] = kurtosis(x)\n",
    "        X.loc[segment, 'quantile_095'] = np.quantile(x,0.95)\n",
    "        X.loc[segment, 'quantile_099'] = np.quantile(x,0.99)\n",
    "        X.loc[segment, 'quantile_09'] = np.quantile(x,0.9)\n",
    "        X.loc[segment, 'quantile_01'] = np.quantile(x,0.1)\n",
    "        X.loc[segment, 'quantile_005'] = np.quantile(x,0.05)\n",
    "        X.loc[segment, 'quantile_001'] = np.quantile(x,0.01)\n",
    "        X.loc[segment, 'std'] = x.std()\n",
    "        X.loc[segment, 'max'] = x.max()\n",
    "        X.loc[segment, 'min'] = x.min()\n",
    "        X.loc[segment, 'abs_max'] = abs(x).max()\n",
    "    \n",
    "    if is_not_submission:\n",
    "        return X, Y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement\n",
    "\n",
    "L'extraction du dataset entier peut prendre du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    data = load_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons le dataset entier afin d'en extraires les features qui nous interessent, nous séparons nos données en sous ensembles de taille 4096 afin de respecter la répartition des données vue précédemment.\n",
    "\n",
    "Cette étape peut aussi prendre beaucoup de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = preproc_data(data, rows=4_096)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous extrayons de ces features l'ensemble d'entrainement et l'ensemble de test. Comme nous n'optimisons aucun hyper paramètre, une séparation naïve tel que prendre les 90% premiers éléments pour l'entrainement est les 10% restant pour le test est donc normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X)*0.9)]\n",
    "Y_train = Y[:int(len(X)*0.9)]\n",
    "X_test = X[int(len(X)*0.9):]\n",
    "Y_test = Y[int(len(X)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons l'outil de scaling par défaut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au départ un modèle svm `NuSVR` avait été utilisé, mais sa complexité quadratique semble être trop lente pour nos ordinateurs. Nous avons donc basculé vers un model de `RandomForestRegressor` qui semble tout autant apprecié par la communauté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = NuSVR()\n",
    "# svm.fit(X_train_scaled, Y_train.values.flatten(), verbose=True)\n",
    "# y_pred = svm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train_scaled, Y_train.values.flatten())\n",
    "y_pred = rfr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étude de l'importance des features montre que kurtonis, le moyennage et l'écart type sont les grandeur les plus importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rfr.feature_importances_,['ave', 'std','kurt', 'max', 'min','abs_max',\n",
    "                                'quantile_095','quantile_099','quantile_09',\n",
    "                                'quantile_01','quantile_005','quantile_001'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fonction de moyennage avait été créée, elle permet de réduire le bruit que l'on peut voir sur la cellule suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array, step):\n",
    "    array_len= len(array)\n",
    "    array_mean = list(array[:])\n",
    "    mean_step = step\n",
    "    for i in range(mean_step,array_len):\n",
    "        array_mean[i] = sum([array[i-j] for j in range(mean_step)])/mean_step\n",
    "    return array_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` n'est pas débruité sur la cellule suivante, on note la grande variance de ses valeur, le moyennage de 36 de ces valeurs permet d'obtenir la figure suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred)\n",
    "plt.plot(Y_test)\n",
    "plt.show()\n",
    "\n",
    "y_pred_mean = mean(y_pred,36)\n",
    "plt.plot(y_pred_mean)\n",
    "plt.plot(Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,40):\n",
    "    y_pred_mean = mean(y_pred,i)\n",
    "    score = mean_absolute_error(Y_test.values.flatten(), y_pred_mean)\n",
    "    print(f'mean: {i}, Score: {score:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without quantile : 2.202, 2.031 (8)\n",
    "# with 5 quantiles : 2.171, 2.023 (8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Certaines notions précédement utilisés sont expliqués ici. En effet, le test de notre algorithme par Kaggle se fait par la prédiction de 2624 valeurs, chacune grâce à un échantillon de 150_000 valeurs. Une seule valeur doit être envoyé pour l'ensemble de ces 150_000 valeurs. Le score est déterminé par une Mean Absolute Error.\n",
    "La solution utilisé a donc été un moyennage. Cette solution est justifiable par le fait que l'on voit que moyenner la prédiction de 36 ensemble ne semble faire que diminuer la MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [['seg_id','time_to_failure']]\n",
    "for test_file in glob.glob('data/test/*'):\n",
    "    test_data = load_data(test_file)\n",
    "    X_train_test = preproc_data(test_data, is_not_submission=False)\n",
    "    X_train_test_scaled = scaler.transform(X_train_test)\n",
    "    y_pred_test = rfr.predict(X_train_test_scaled)\n",
    "    y_pred_test_value = sum(y_pred_test)/len(y_pred_test)\n",
    "    results.append([test_file.split('/')[2], y_pred_test_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous mettons en forme le résultat pour l'envoyer à Kaggle. Notre erreur MAE annoncée est de 2.257 et notre classement est 2324 sur 2700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    result[0] = result[0].split('.')[0]\n",
    "with open('result.csv','w+') as f:\n",
    "    f.write('\\n'.join([','.join([str(rr) for rr in r]) for r in results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amélioration\n",
    "\n",
    "Une solution pour augmenter notre score a été d'utiliser NuSVR et entrainer notre ensemble de donnée sur la totalité de l'ensemble de test, donc 150_000 points, notre MAE annoncée passe alors à 1.639 et notre classement monte 1853ème : https://www.kaggle.com/c/LANL-Earthquake-Prediction/leaderboard (nous sommes CS-DAC-Team)\n",
    "\n",
    "On pourra expliquer cette situation par l'importance que doivent avoir les valeur extrême et qui sont étouffés par la moyenne, ce qui semble néanmoins étonnant au regard de la MAE.\n",
    "\n",
    "Celà est d'autant plus étonnant que notre MAE personnelle est d'environ 2.1 en prenant des sample de 4096 valeurs et qu'elle monte à 2.3 en prenant des sample de 150_000 valeurs.\n",
    "\n",
    "Augmenter notre nombre de features et/ou faire de la grid search sur les hyperparamètre de notre modèle de RandomTreeRegressor pourrait nous permettre d'améliorer ce score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
